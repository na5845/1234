#!/usr/bin/env python3
"""
×ª××œ×•×œ ××¨×•×‘×” ×§×‘×¦×™× ×¢× Whisper
×©×™××•×©: python batch_transcribe.py *.mp3
"""

import whisper
import os
import sys
import glob
import argparse
from pathlib import Path
from datetime import datetime
import json
from tqdm import tqdm
import concurrent.futures

def transcribe_file(file_path, model, output_dir):
    """×ª××œ×œ ×§×•×‘×¥ ×‘×•×“×“"""
    try:
        print(f"\nğŸ™ï¸ ××ª××œ×œ: {os.path.basename(file_path)}")
        
        # ×ª××œ×œ
        result = model.transcribe(file_path, language="he")
        
        # ×¦×•×¨ ×©××•×ª ×§×‘×¦×™×
        base_name = Path(file_path).stem
        txt_path = os.path.join(output_dir, f"{base_name}.txt")
        srt_path = os.path.join(output_dir, f"{base_name}.srt")
        json_path = os.path.join(output_dir, f"{base_name}.json")
        
        # ×©××•×¨ ×˜×§×¡×˜
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(result["text"])
        
        # ×©××•×¨ SRT
        with open(srt_path, "w", encoding="utf-8") as f:
            for i, segment in enumerate(result["segments"], 1):
                start = format_time(segment["start"])
                end = format_time(segment["end"])
                f.write(f"{i}\n{start} --> {end}\n{segment['text'].strip()}\n\n")
        
        # ×©××•×¨ JSON
        metadata = {
            "file": file_path,
            "date": datetime.now().isoformat(),
            "duration": result.get("duration", 0),
            "text": result["text"],
            "segments": result["segments"]
        }
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return True, file_path, None
        
    except Exception as e:
        return False, file_path, str(e)

def format_time(seconds):
    """×”××¨ ×©× ×™×•×ª ×œ×¤×•×¨××˜ SRT"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace(".", ",")

def main():
    parser = argparse.ArgumentParser(description='×ª××œ×•×œ ××¨×•×‘×” ×§×‘×¦×™×')
    parser.add_argument('files', nargs='+', help='×§×‘×¦×™ ××•×“×™×• ×œ×ª××œ×•×œ')
    parser.add_argument('--model', default='base', 
                       choices=['tiny', 'base', 'small', 'medium', 'large'],
                       help='×’×•×“×œ ×”××•×“×œ')
    parser.add_argument('--output', default='batch_output', 
                       help='×ª×™×§×™×™×ª ×¤×œ×˜')
    parser.add_argument('--parallel', type=int, default=1,
                       help='××¡×¤×¨ ×ª××œ×•×œ×™× ×‘××§×‘×™×œ (×‘×¨×™×¨×ª ××—×“×œ: 1)')
    
    args = parser.parse_args()
    
    # ××¡×•×£ ×§×‘×¦×™×
    all_files = []
    for pattern in args.files:
        if '*' in pattern or '?' in pattern:
            all_files.extend(glob.glob(pattern))
        else:
            all_files.append(pattern)
    
    # ×¡× ×Ÿ ×¨×§ ×§×‘×¦×™× ×§×™×™××™×
    valid_files = [f for f in all_files if os.path.exists(f)]
    
    if not valid_files:
        print("âŒ ×œ× × ××¦××• ×§×‘×¦×™× ×œ×ª××œ×•×œ")
        sys.exit(1)
    
    print(f"ğŸ“ × ××¦××• {len(valid_files)} ×§×‘×¦×™× ×œ×ª××œ×•×œ")
    
    # ×¦×•×¨ ×ª×™×§×™×™×ª ×¤×œ×˜
    os.makedirs(args.output, exist_ok=True)
    print(f"ğŸ“‚ ×ª×™×§×™×™×ª ×¤×œ×˜: {args.output}")
    
    # ×˜×¢×Ÿ ××•×“×œ
    print(f"\nğŸ”„ ×˜×•×¢×Ÿ ××•×“×œ {args.model}...")
    model = whisper.load_model(args.model)
    
    # ×”×ª×—×œ ×ª××œ×•×œ
    print(f"\nğŸš€ ××ª×—×™×œ ×ª××œ×•×œ ×©×œ {len(valid_files)} ×§×‘×¦×™×...")
    
    results = []
    failed = []
    
    # ×ª××œ×•×œ ×¢× progress bar
    with tqdm(total=len(valid_files), desc="×ª××œ×•×œ", unit="×§×•×‘×¥") as pbar:
        if args.parallel > 1:
            # ×ª××œ×•×œ ××§×‘×™×œ×™
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.parallel) as executor:
                futures = []
                for file_path in valid_files:
                    future = executor.submit(transcribe_file, file_path, model, args.output)
                    futures.append(future)
                
                for future in concurrent.futures.as_completed(futures):
                    success, file_path, error = future.result()
                    if success:
                        results.append(file_path)
                    else:
                        failed.append((file_path, error))
                    pbar.update(1)
        else:
            # ×ª××œ×•×œ ×¡×“×¨×ª×™
            for file_path in valid_files:
                success, file_path, error = transcribe_file(file_path, model, args.output)
                if success:
                    results.append(file_path)
                else:
                    failed.append((file_path, error))
                pbar.update(1)
    
    # ×¡×™×›×•×
    print("\n" + "="*50)
    print("ğŸ“Š ×¡×™×›×•× ×ª××œ×•×œ:")
    print("="*50)
    print(f"âœ… ×”×¦×œ×™×—×•: {len(results)} ×§×‘×¦×™×")
    print(f"âŒ × ×›×©×œ×•: {len(failed)} ×§×‘×¦×™×")
    
    if failed:
        print("\nğŸ”´ ×§×‘×¦×™× ×©× ×›×©×œ×•:")
        for file_path, error in failed:
            print(f"  - {file_path}: {error}")
    
    # ×¦×•×¨ ×§×•×‘×¥ ×¡×™×›×•×
    summary_file = os.path.join(args.output, "summary.json")
    summary = {
        "date": datetime.now().isoformat(),
        "model": args.model,
        "total_files": len(valid_files),
        "successful": len(results),
        "failed": len(failed),
        "results": results,
        "errors": [{"file": f, "error": e} for f, e in failed]
    }
    
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ ×¡×™×›×•× × ×©××¨ ×œ: {summary_file}")

if __name__ == "__main__":
    main()
